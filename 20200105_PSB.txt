10-min talk: 1200 words

Good morning! My name is Irene Chen, a 4th year PhD student in MIT, working by David Sontag. Today I'll presenting our work on building robust health knowledge graphs.

[0:00 - 2:00]
With the prevalence of automated diagnostic services like Babylon Health, we are increasingly interested in building a health knowledge graph between diseases and symptoms. 

Here we have three example diseases and four symptoms. Pop quiz: What symptoms would we imagine a migraine would cause? That's right, headache and nausea. Similarly, lyme disease causes headache and skin rash. And endometriosis causes nausea and abdominal pain.

[1:00]

Historically, we have learned medical knowledge like a diagnostic health knowledge graph through a few means, for example experimentation on animal models like mice and observation of abnormal conditions as they appear in humans. However, these sources can be flawed or limited in scope. As researchers, we are excited to bring in an additional source of information: electronic health records. The hope is that the immense size and increased availability of electronic health records would lead to more accurate models.

However, the use of EHRs raises questions about robustness. 

For example, subpopulations in a dataset can have very different summary statistics. Here we see key differences in rural hospital inpatients compared to urban hospital inpatients according to the CDC -- namely the age distribution.

We may also be relying on signal that is not as clinically meaningful. Using the open access CheXpert dataset of 220k chest xrays, researchers look to predict pneumothorax from a chest xray. Interestingly, a model predicting pneumothorax yielded a very high AUC of 94% on patients with a chest drain compared to AUC of 77% on patients without one. Because a chest drain can be used to treat pneumothorax, we might be more concerned about patients without a chest drain and results on patients with a chest drain might be less important.

[2:30]

As we build and deploy more models for clinical machine learning, it is important then that models work across populations and subpopulations and that we understand any confounders that might impact the clinical application.

[A tale of two papers]

Which brings us to the key question. How could we robustly extract medical knoweldge from EHRs? Specifically, how can we evaluate a health knowledge graph for robustness?

Our paper builds on our prior work published in Nature Scientific Reports in 2017 in order to robustly evaluate the published model and results. 

[3:00]

First some background on the original paper. Using data from Beth Israel Deaconness Medical Center in Boston, we collected emergency department data from 2008-2013. ***

From each patient visit to the emergency department, we extracted positive mentions of diseases and symptoms from the structured diagnoses codes and the unstructured clinical notes. Note that some patients visited the emergency department multiple times, so each visit is treated separately.

The goal is then to learn a binary matrix relating diseases and symptoms with a 1 if a disease causes a symptom and a 0 otherwise. For example, here is the earlier example graph. 

Using three different methods -- logistic regression, naive bayes, and noisy or -- we can learn three different health knowledge graph from 270k patient records. How can we determine which health knowledge graph is better?

We evaluate using a Google Health Knowledge Graph which was curated by Google from clinical experts on a subset of diseases and symptoms. If you Google "headache nausea", parts of this Google Health Knowledge Graph is exposed on the panel as listing diseases with known symptoms.

[4:45]

In collaboration with our colleagues at Google, we gained access to this health knowldge graph for an automated evaluation method.

We can measure precision and recall against this subset of diseases and symptoms. 

For each disease, we compare the symptoms listed by the Google Health Knowledge Graph and our health knowledge graph ranking and report precision, recall, and combined F1 score. Of the three models, the Noisy Or model performed the best.

The final derived Health Knowledge Graph was validated manually by clinicians and achieved a precision of 0.85 and recall of 0.6. 

[Our paper]

In our paper, we are interested in robustness for the health knowledge graph. 

After all, this health knowledge presents the first step in an automated diagnostic network that could help 1) train clinicians, 2) assist clinicial workflows with diagnostic suggestions, and 3) substitution in narrowly defined tasks like triaging patients. It is imoprtant then that any model be thoroughly evaluated for performance and robustness.

As a sneak peak of our paper, we look into the robustness of the health knowledge graph.

1) Not all datasets are created equal. How do factors like dataset size, granularity, and which features are included perform? For example, the original paper used data from the emergency department -- what if we data from the entire patient record instead?

2) In order to be clinicially meaningful, we want edges in the health knowledge graph that are causal. Because we are using a bipartite graph structure, we may be ignoring potential confounders -- for example diseases that cause other diseases, or a patient's age causing both the disease and the symptom. We assess the model for potential confounders through error analysis.

3) Lastly, we present nonlinear causal methods and evaluate them against the existing model. 

[7:00]

[Ending]

In conclusion, as we think about future research, our paper presents three main ideas. 

First, if you enjoyed Toy Story 2 or The Dark Knight, you know that sequels can be fun. In a research context, it is important for us to evaluate prior work in order to ensure nuanced understanding before clinical deployment.

Second, robustness is a broad concept, so it is important to consider over which factors we have control: the dataset cohort, the subpopulations used, and the model we ultimately use.

Lastly, for those of you who are interested in health diagnostics, the Noisy Or was ultimately incredibly robust to many tests. Please see our paper for more details.